# -*- coding: utf-8 -*-
"""410_sentiment_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18--dK8gFtzaWItBEU8mb-RYWaTyuivxI
"""

from pprint import pprint
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import display, HTML
import nltk
nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer as S
from nltk.tokenize import word_tokenize
nltk.download('punkt')
import re
from nltk.corpus import wordnet as wn
nltk.download('wordnet')

"""In this section of code we are installing the necessary packages required to perform the sentiment analysis, and the necessary preprocesses to acquire the text data that we are looking to gather from the subreddits from reddit. This information is all in the requirements.txt as well.

"""

# ! pip install praw
import praw

CSS = """
.output {
    flex-direction: row;
}
"""

HTML('<style>{}</style>'.format(CSS))

user_agent = "Scraper 1.0 by /u/Alternative_Elk2555"
reddit = praw.Reddit(
    client_id = "ggBGWPlZTGbJWFmr0nyiRw",
    client_secret = "vk_rAmVVOihqxWJDo1AnfyHbEEIXvA",
    user_agent = user_agent,
    check_for_async=False
)

"""In the two sections of code above we are installing and setting up the web crawler for the subreddits so that way we have a means of extracting the content of the subreddit posts."""

headlines_uiuc = []
comments_uiuc = []
for submission in reddit.subreddit('UIUC').hot(limit = 500):
  # print(submission.title, "title")
  comments = []
  for comment in submission.comments:
    # print(comment.body, "body")
    comments.append(comment.body)
  headlines_uiuc.append(submission.title)
  # print(headlines_uiuc, "headline")
  comments_uiuc.append(comments)
  # print(comments_uiuc, "comment")
# print(comments_uiuc, "comments_uiuc")

headlines_chicago = []
comments_chicago = []
for submission in reddit.subreddit('chicago').hot(limit = 500):
  # print(submission.title, ",title")
  comments = []
  for comment in submission.comments:
    if hasattr(comment, 'body'):
      comments.append(comment.body)
    else:
      continue
  headlines_chicago.append(submission.title)
  comments_chicago.append(comments)
# print(len(headlines_chicago))

"""In this section of code we are extracting all of the posts from each respective subreddit so that way we have access to the actual contents of the subreddits to use when later on we filter for posts respective to food. This is being done with the webscraper and we make sure to grab both the comments and the titles for each post so they can be analyzed later

"""

print(len(headlines_uiuc))
print(len(comments_uiuc))
print(len(headlines_chicago))
print(len(comments_chicago))
df = pd.DataFrame({'headline':headlines_uiuc, 'comments':comments_uiuc})
df['most_association'] = 0

for index, row in df.iterrows():
  title = row[0]
  # print(title)
  # remove stopwords
  title = re.sub("[\[].*?[\]]", "", title)
  title =re.sub(r"[^a-zA-Z.]+", ' ', title)
  tokenize_list = word_tokenize(title)
  # print(tokenize_list)
  if len(tokenize_list) == 0:
    continue

  total_association = 0
  for word in tokenize_list:
    word1 = word
    word2 = "taste"
    if len(wn.synsets(word1)) == 0:
      continue
    syn1 = wn.synsets(word1)[0]
    syn2 = wn.synsets(word2)[0]
    value = syn1.wup_similarity(syn2)
    if value > total_association:
      total_association = value

  df.at[index, 'most_association'] = total_association
df = df.sort_values(by=['most_association'], ascending=False)
df = df.head(50)



# Starting filtering of chicago reddit posts
df2 = pd.DataFrame({'headline':headlines_chicago, 'comments':comments_chicago})
df2['most_association'] = 0

for index, row in df2.iterrows():
  title = row[0]
  print(title)
  # remove stopwords
  title = re.sub("[\[].*?[\]]", "", title)
  title =re.sub(r"[^a-zA-Z.]+", ' ', title)
  tokenize_list = word_tokenize(title)
  print(tokenize_list)
  if len(tokenize_list) == 0:
    continue

  total_association = 0
  for word in tokenize_list:
    word1 = word
    word2 = "food"
    if len(wn.synsets(word1)) == 0:
      continue
    syn1 = wn.synsets(word1)[0]
    syn2 = wn.synsets(word2)[0]
    value = syn1.wup_similarity(syn2)
    if value > total_association:
      total_association = value

  df2.at[index, 'most_association'] = total_association
df2 = df2.sort_values(by=['most_association'], ascending=False)
df2 = df2.head(50)

# displaying both dfs
display(df)
display(df2)

"""In this section we are filtering the posts by using the token of "taste" and "food" to filter for posts that pertain to that topic, we used the token of "taste" & "food" because we found that keyword had the best results pertaining to food. Essentially, we check the similarity score of the title of each post with the words food and taste to see how relevant they are for our idea. If they contain either word or have a high similarity score, they will likely be used for the next phase of our project.
We implemented this through the use of tokenization.
"""

# starting sentiment analysis
s_uiuc = S()
results_uiuc = []

s_chicago = S()
results_chicago = []

for index, row in df.iterrows():
  line = row[0]
  polarity = s_uiuc.polarity_scores(line)
  polarity['headline'] = line
  polarity['comments'] = row[1]
  results_uiuc.append(polarity)

for index, row in df2.iterrows():
  line = row[0]
  polarity = s_chicago.polarity_scores(line)
  polarity['headline'] = line
  polarity['comments'] = row[1]
  results_chicago.append(polarity)

pprint(results_uiuc[:3], width = 100)
print("\n")
print("\n")
pprint(results_chicago[:3], width = 100)

"""In this section of code we are performing sentiment analysis from the respective posts that we have obtained earlier through the use of tokenization, we obtained the sentiment analysis through the use of polarization which allowed for us to identify if posts were negative, neutral, and positive as indicated within the output. We did this for each respective subreddit: UIUC and chicago, from which now we have a baseline for the tone of the posts which will then allow us to utilize this data later on to assess the top rated posts."""

df = pd.DataFrame.from_records(results_uiuc)
df.head()

df2 = pd.DataFrame.from_records(results_chicago)
df2.head()

display(df)
display(df2)

"""This section of code we created a dataframe for each respective subreddit from which we then take the top results, in the output above you can see that there are neg, neu, pos, and compound fields with values indicating the respective correlation to each label. From this we have actually identified our first baseline of sentiment analysis, in the next segment we will actually begin filtering the results in order to identify the posts with either positive or negative labels by utilizing the compound score that we have found in order to do so."""

df['label'] = 'neutral'
df.loc[df['compound'] > 0.2, 'label'] = 'positive'
df.loc[df['compound'] < -0.2, 'label'] = 'negative'
df.head()

df_main = df[['headline', 'comments', 'label', 'compound']]
df_main


df2['label'] = 'neutral'
df2.loc[df2['compound'] > 0.2, 'label'] = 'positive'
df2.loc[df2['compound'] < -0.2, 'label'] = 'negative'
df2.head()

df2_main = df2[['headline', 'comments', 'label', 'compound']]
df2_main

display(df_main)
display(df2_main)

"""In this section of code we have actually identified if a post is deemed either positive or negative by utilizing a threshold value of 0.2 in the compound field. Essentially, what that means is if a post has a compound value higher than 0.2 then it is deemed positive, negative otherwise. From this we now have actually identified the positive and negative opinions about food within each subreddit, in the next code segments we will then begin to explore this text data further."""

# comments associated with positive sentiments about food

uiuc_comments = []
for index, row in df.iterrows():
  comments = row[5]
  # print(comments)
  if len(comments) == 0:
    continue
  for comment in comments:
    full_comment = comment
    # print(comment)
    # remove stopwords
    comment = re.sub("[\[].*?[\]]", "", comment)
    comment =re.sub(r"[^a-zA-Z.]+", ' ', comment)
    tokenize_list = word_tokenize(comment)
    # print(tokenize_list)
    if len(tokenize_list) == 0:
      continue
    for word in tokenize_list:
      word1 = word
      word2 = "food"
      if len(wn.synsets(word1)) == 0:
        continue
      syn1 = wn.synsets(word1)[0]
      syn2 = wn.synsets(word2)[0]
      value = syn1.wup_similarity(syn2)
      if 0.8 < value and row[3] > 0.2:
        uiuc_comments.append(full_comment)
        break
print(uiuc_comments, "final comments")


chicago_comments = []
for index, row in df2.iterrows():
  comments = row[5]
  #print(comments)
  if len(comments) == 0:
    continue
  for comment in comments:
    full_comment = comment
    #print(comment)
    # remove stopwords
    comment = re.sub("[\[].*?[\]]", "", comment)
    comment =re.sub(r"[^a-zA-Z.]+", ' ', comment)
    tokenize_list = word_tokenize(comment)
    #print(tokenize_list)
    if len(tokenize_list) == 0:
      continue
    for word in tokenize_list:
      word1 = word
      word2 = "food"
      if len(wn.synsets(word1)) == 0:
        continue
      syn1 = wn.synsets(word1)[0]
      syn2 = wn.synsets(word2)[0]
      value = syn1.wup_similarity(syn2)
      if 0.8 < value and row[3] > 0.2:
        chicago_comments.append(full_comment)
        break
print(chicago_comments, "final comments")

"""In this section of code we have filtered the data to obtain all of the positive food opinions within each subreddit and we have used the token "food" in order to display these positive opinions, and in doing so the output reflects the respective opinions about food within the posts of each subreddit, from which we can leverage this data further to explore interesting findings that we have found."""

display(df.label.value_counts(normalize = True) * 100)
print("\n")
display(df2.label.value_counts(normalize = True) * 100)

"""In this section of code have found the value counts for each subreddit and displayed the percentages of the opinions that we have obtained for the subreddits UIUC and chicago respectively in regards to their opinions about food."""

display(df.groupby('label')['compound'].describe())
display(df2.groupby('label')['compound'].describe())

"""In this section of code we are displaying the respective compound values for each dataframe and their label values, from which we can see the total amount of posts for each label and the statistics found for the label within each subreddit."""

df.boxplot(by='label', column='compound', figsize=(12,8))
df2.boxplot(by='label', column='compound', figsize=(12,8))

"""We have now displayed a visualization of the findings that we have obtained from the text data, so that you can clearly see the different relationships that each subreddit has alongside food from which we can see that chicago had more variability when it came to negative food whereas UIUC had more variability when it comes to positive food reviews, but overall the opinions in relation to food appear to be relatively similar when we compare both of the cities, as illustrated within the boxplot."""

